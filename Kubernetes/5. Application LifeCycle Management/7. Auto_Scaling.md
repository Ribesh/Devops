# 2025 Updates Introduction to Autoscaling

![](../../images/kubernetes_auto_scaling.png)

## Traditional Scaling Concepts

-   Historically, applications were deployed on physical servers with **fixed CPU and memory capacities**. 

-   When demand increased and resources were exhausted, the only option was to perform vertical scaling. This involved:

    1.  Shutting down the application.
    2.  Upgrading the CPU or memory.
    3.  Restarting the server.

-   This process is referred to as **vertical scaling** since it focuses on enhancing the capacity of an existing server.

-  Conversely, if an application supported multiple instances, additional servers could be added to handle increased loads without any downtime. This method, known as **horizontal scaling**, distributes the workload by creating more instances of the application.

## Key Points:

1. **Vertical Scaling:** Increases resources (CPU, memory) of an existing server.

2. **Horizontal Scaling:** Increases server count by adding more instances.

![](../../images/kubernetes_auto_scaling1.png)

## Scaling in Kubernetes

-   Kubernetes is specifically designed for hosting containerized applications and incorporates scaling based on current demands. **It supports two main scaling types:**
    1. **Workload Scaling:** Adjusting the number of containers or Pods running in the cluster.
    2. **Cluster (Infrastructure) Scaling**: Adding or removing nodes (servers) from the cluster.

![](../../images/kubernetes_auto_scaling2.png)


When scaling in a Kubernetes cluster, consider the following:

-  **1. Cluster Infrastructure Scaling:**

    -   *Horizontal Scaling*: Add more nodes.

    -   *Vertical Scaling*: Enhance the resources (CPU, memory) of existing nodes.

-  **2. Workload Scaling:**
    -   *Horizontal Scaling*: Create additional Pods.
        
    -   *Vertical Scaling*: Modify the resource limits and requests for existing Pod


![](../../images/kubernetes_auto_scaling3.png)

## Approaches to Scaling in Kubernetes
Kubernetes supports both manual and automated scaling methods.

### Manual Scaling
Manual scaling requires intervention from the user:

#### 1.  Infrastructure Scaling (Horizontal): 
Provision new nodes and join them to the cluster using:
```bash
kubectl join ...
```

#### 2. Workload Scaling (Horizontal): 
Adjust the number of Pods manually with:
    
```bash
kubectl scale ...
```

#### 3. Pod Resource Adjustment (Vertical):
Edit the deployment, stateful set, or replica set to modify resource limits and requests:

```bash
kubectl edit ...
```

### Automated Scaling
Automation in Kubernetes simplifies scaling and ensures efficient resource management:

#### 1. Kubernetes Cluster Autoscaler
-   Automatically **adjusts the number of nodes** in the cluster by adding or removing nodes when needed.

#### 2. Horizontal Pod Autoscaler (HPA):
-   Monitors metrics and **adjusts the number of Pods dynamically.**

#### 3. Vertical Pod Autoscaler (VPA):
-   Automatically **changes resource allocations for running Pods based on observed usage.**


### Key Benefit

Automated scaling mechanisms in Kubernetes allow your applications and infrastructure to adapt quickly to changing loads, reducing manual effort and ensuring optimal performance.